import torchfrom torch.utils.data import Dataset, DataLoaderfrom torchvision import transformsimport jsonimport numpy as npimport cv2class KeypointsDataset(Dataset):    def __init__(self, img_dir, data_file):        self.img_dir = img_dir        with open(data_file, "r") as f:            self.data = json.load(f)        self.transforms = transforms.Compose(            [                transforms.ToPILImage(),                transforms.Resize((224, 224)),                transforms.ToTensor(),                transforms.Normalize(                    mean=[0.485, 0.456, 0.406],                    std=[0.229, 0.224, 0.225],                ),            ]        )    def __len__(self):        return len(self.data)    def __getitem__(self, idx):        item = self.data[idx]        img = cv2.imread(f"{self.img_dir}/{item['id']}.png")        h, w = img.shape[:2]        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # convert image to RGB        img = self.transforms(img)        kps = item["kps"]  # get the keypoints for the image        kps = np.array(item["kps"]).flatten()  # flatten the array        kps = kps.astype(np.float32)        # Normalize coordinates to match the 224x224 image size        kps[::2] *= 224.0 / w  # adjust x coordinates        kps[1::2] *= 224.0 / h  # adjust y coordinates        return img, kpsdef get_tennis_kps_dataset():    parent_path = "/Users/parteeksj/Desktop/DATASETS/tennis_court_det_dataset"    train_dataset = KeypointsDataset(        img_dir=parent_path + "/images", data_file=parent_path + "/data_train.json"    )    val_dataset = KeypointsDataset(        img_dir=parent_path + "/images", data_file=parent_path + "/data_val.json"    )    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)    return train_loader, val_loaderif __name__ == "__main__":    parent_path = "/Users/parteeksj/Desktop/DATASETS/tennis_court_det_dataset"    train_dataset = KeypointsDataset(        img_dir=parent_path + "/images", data_file=parent_path + "/data_train.json"    )    val_dataset = KeypointsDataset(        img_dir=parent_path + "/images", data_file=parent_path + "/data_val.json"    )    img, kps = train_dataset.__getitem__(0)    print(f"{img.shape=}")    print(f"{kps.shape=}")    print(f"{train_dataset.__len__()=}")  # 6630    print(f"{val_dataset.__len__()=}")  # 6630"""Tennis Court Keypoints Dataset can be downloaded in Google Colab as followsDataset Link = https://drive.google.com/file/d/1lhAaeQCmk2y440PmagA0KmIVBIysVMwu/view?usp=drive_link    # Install gdown!pip install --quiet gdown# Import gdownimport gdown# Download the filefile_id = '1lhAaeQCmk2y440PmagA0KmIVBIysVMwu'gdown.download(id=file_id, output='dataset.zip', quiet=False)# Unzip the fileimport zipfilewith zipfile.ZipFile('dataset.zip', 'r') as zip_ref:    zip_ref.extractall('dataset')        # Dataset Path: '/content/dataset/data'"""